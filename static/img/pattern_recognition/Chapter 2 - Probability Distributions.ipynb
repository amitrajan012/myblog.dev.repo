{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One goal of pattern recognition is to model the <b>probability distribution</b> $p(x)$ of a random variable $x$ given the finite set of points $x_1, x_2, ..., x_N$. This problem is known as <b>density estimation</b>. For simplicity, we can assume that the points are <b>independent and identically distributed</b>. There can be infinitely many distributions that can give rise to the given data points with any distribution that is non-zero at the points $x_1, x_2, ..., x_N$ as a potential candidate. \n",
    "\n",
    "A <b>parametric distribution</b> is the distribution which is governed by a set of parameters. For example, a gaussian distribution is governed by its mean and variance. Hence, to model a distribution, we need a procedure for determining the suitable values for these parameters. There are two ways to do it: <b>Frequentist</b> and <b>Bayesian</b> approach. In a frequentist approach, we determine the values of parameters by optimizing some criteria like the likelihood function, i.e. we have to find a parameter set $w$ which will maximize the probability of observing the given data points $D$ ($p(D|w)$). In a Bayesian setting, we assume some prior distribution over the parameters and then use Bayes' theorem to compute the corresponding posterior distribution given the observed data points $D$. If we assume that the posterior distribution takes the same form as prior, the analysis for Bayesian approach will be greatly simplified. \n",
    "\n",
    "One major limitation of parametric approach is that we assume a specific functional form of the distribution, which may turn out to be inappropriate for a particular application. An alternative approach is given by a <b>nonparametric density estimation</b> methods where the form of the distribution depends on the size of the dataset and it's parameter usually controls the model complexity.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Binary Variables\n",
    "\n",
    "Let us consider a simple coin flip experiment where we have the set of outcomes defined as $x = \\{0,1\\}$, with $x=1$ represnting heads and $x=0$ representing tails. The probability of $x=1$ will be denoted by a parameter $\\mu$ where $0 \\leq \\mu \\leq 1$ as:\n",
    "\n",
    "$$\\begin{align}\n",
    "p(x=1|\\mu) = \\mu\n",
    "\\end{align}$$\n",
    "\n",
    "This gives us the probability of $x=0$ as $p(x=0|\\mu) = 1-\\mu$. The probability distribution over $x$ can be given as:\n",
    "\n",
    "$$\\begin{align}\n",
    "Bern(x|\\mu) = \\mu^x(1-\\mu)^{1-x}\n",
    "\\end{align}$$\n",
    "\n",
    "and is called as <b>Bernoulli Distribution</b>. The distribution is normalized and its mean and variance are given as:\n",
    "\n",
    "$$\\begin{align}\n",
    "E[x] = \\mu; Var[x] = \\mu(1 - \\mu)\n",
    "\\end{align}$$\n",
    "\n",
    "For a dataset $D = \\{x_1, x_2, ..., x_N\\}$, assuming the the points are drawn independently from $p(x\\mu)$, the likelihoof function $p(D|\\mu)$ can be defined as:\n",
    "\n",
    "$$\\begin{align}\n",
    "p(D|\\mu) = \\prod_{n=1}^{N} p(x_n|\\mu) = \\prod_{n=1}^{N}\\mu^{x_n}(1-\\mu)^{1-x_n}\n",
    "\\end{align}$$\n",
    "\n",
    "In a frequentist setting, we can estimate the value of parameter $\\mu$ by maximizing the likelihood function $p(D|\\mu)$ or maximizing the log of the likelihood function instead. For Bernoulli distribution, the log likelihood function is given as:\n",
    "\n",
    "$$\\begin{align}\n",
    "\\ln p(D|\\mu) = \\sum_{n=1}^{N} \\ln p(x_n|\\mu) = \\sum_{n=1}^{N} [x_n \\ln \\mu + (1-x_n)\\ln (1 - \\mu)]\n",
    "\\end{align}$$\n",
    "\n",
    "The <b>maximum likelihood estimator</b> of $\\mu$ can be determined by setting the derivative of $\\ln p(D|\\mu)$ with respect to $\\mu$ equal to zero and is derived as:\n",
    "\n",
    "$$\\begin{align}\n",
    "\\frac{\\delta\\ln p(D|\\mu)}{\\delta\\mu} = \\sum_{n=1}^{N} \\bigg[\\frac{x_n}{\\mu} - \\frac{(1-x_n)}{(1 - \\mu)}\\bigg] = 0\n",
    "\\end{align}$$\n",
    "\n",
    "$$\\begin{align}\n",
    "\\implies \\sum_{n=1}^{N} \\bigg[(1 - \\mu)x_n - \\mu(1-x_n)\\bigg] = 0\n",
    "\\end{align}$$\n",
    "\n",
    "$$\\begin{align}\n",
    "\\implies \\sum_{n=1}^{N} \\bigg[x_n - \\mu\\bigg] = 0\n",
    "\\end{align}$$\n",
    "\n",
    "$$\\begin{align}\n",
    "\\implies \\mu_{ML} = \\frac{1}{N}\\sum_{n=1}^{N} x_n\n",
    "\\end{align}$$\n",
    "\n",
    "which is the <b>sample mean</b>. If the number of observations with $x=1$ is $m$, the the MLE estimator of the parameter $\\mu$ is given as:\n",
    "\n",
    "$$\\begin{align}\n",
    "\\mu_{ML} = \\frac{m}{N}\n",
    "\\end{align}$$\n",
    "\n",
    "One of the major darwaback of the frequentist approach is lets say we just have $5$ data points and for all those data points, the outcome is head, i.e. $m=5,N=5$. This means that the maximum likelihood estimator of mean is $\\mu_{ML} = \\frac{5}{5} = 1$. Hence the maximum likelihood estimator will predict that all the future outcomes will be head. This is an extreme example of overfitting associated with maximum likelihood estimator.\n",
    "\n",
    "Another important distribution is <b>Binomial Distribution</b>. It is the distribution for the number of observations for which $x=1$ given that we have a total of $N$ data points (number of heads in $N$ coin tosses). Binomial distribution is given as:\n",
    "\n",
    "$$\\begin{align}\n",
    "Bin(m|N,\\mu) = {N \\choose m} \\mu^m (1-\\mu)^{N-m}\n",
    "\\end{align}$$\n",
    "\n",
    "where\n",
    "\n",
    "$$\\begin{align}\n",
    "{N \\choose m} = \\frac{N!}{m!(N-m)!}\n",
    "\\end{align}$$\n",
    "\n",
    "$m=x_1+x_2+...+x_N$ where $x_i$ is a bernoulli variable. For independent events, the mean and variance of sum is the sum of the means and variances. Hence,\n",
    "\n",
    "$$\\begin{align}\n",
    "E[m] = N\\mu\n",
    "\\end{align}$$\n",
    "\n",
    "$$\\begin{align}\n",
    "Var[m] = N\\mu(1-\\mu)\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.1 Beta Distribution\n",
    "\n",
    "As explained above, frequentist approach can lead to highly overfitted results in some of the cases (mainly when the size of dataset is samll). In order to treat the problem with the Bayesian approach, we need to define a prior probability $p(\\mu)$ over the parameter $\\mu$. As posterior probability is the product of prior and the likelihood function and likelihoof function takes the form of product of factors $\\mu^x(1-\\mu)^{1-x}$, if we choose a prior to be proportional to the powers of $\\mu$ and $1-\\mu$, the posterior distribution will have the same functional form. This property is called <b>conjugacy</b> and the prior and posterior distributions are called <b>conjugate distributions</b> with prior distribution being <b>conjugate prior</b> of the likelihood function. <b>Beta Distribution</b> has this property and is given as:\n",
    "\n",
    "$$\\begin{align}\n",
    "Beta(\\mu|a,b) = \\frac{\\Gamma(a+b)}{\\Gamma(a)\\Gamma(b)} \\mu^{a-1}(1-\\mu)^{b-1}\n",
    "\\end{align}$$\n",
    "\n",
    "where $\\Gamma(x)$ is the <b>gamma function</b> defined as:\n",
    "\n",
    "$$\\begin{align}\n",
    "\\Gamma(x) = \\int_{0}^{\\infty} u^{x-1}e^{-u}du\n",
    "\\end{align}$$\n",
    "\n",
    "Beta distribution is normalized with its mean and variance given as:\n",
    "\n",
    "$$\\begin{align}\n",
    "E[\\mu] = \\frac{a}{a+b}\n",
    "\\end{align}$$\n",
    "\n",
    "$$\\begin{align}\n",
    "Var[\\mu] = \\frac{ab}{(a+b)^2(a+b+1)}\n",
    "\\end{align}$$\n",
    "\n",
    "The parameters $a,b$ are <b>hyperparameters</b> as they control the distribution of parameter $\\mu$. The PDF and CDF of Beta distribution is shown below with $a,b$ denoted as $\\alpha, \\beta$ respectively.\n",
    "\n",
    "<img src=\"beta.png\">\n",
    "\n",
    "For a binomial likelihood function where $l=N-m$, the posterior distribution with beta prior is given as:\n",
    "\n",
    "$$\\begin{align}\n",
    "Beta(\\mu|m,l,a,b) \\propto \\mu^{m+a-1}(1-\\mu)^{l+b-1}\n",
    "\\end{align}$$\n",
    "\n",
    "The posterior distribution has the same functional form as the prior and hence beta distribution maintains the property of conjugate prior with respect to likelihood function. The normalization coefficient of posterior distribution can be obtained by comparing it with prior and which gives the posterior distribution as:\n",
    "\n",
    "$$\\begin{align}\n",
    "Beta(\\mu|m,l,a,b) = \\frac{\\Gamma(m+a+l+b)}{\\Gamma(m+a)\\Gamma(l+b)} \\mu^{m+a-1}(1-\\mu)^{l+b-1}\n",
    "\\end{align}$$\n",
    "\n",
    "The effect of observing $m$ observations of $x=1$ and $l$ observations of $x=0$ is the increase in the value of $a$ by $m$ and that of $b$ by $l$. The hyperparametrs $a,b$ can be simply interpreted as the <b>effective number of observations</b> of $x=1$ and $x=0$. This means that the <b>posterior distribution can act like prior if we observe additional data</b>. Hence, we can take observations one at a time and can update the posterior distribution by updating the parameters and normalizing constant. This <b>sequential approach</b> of learning is one of the key property of Bayesian method. This sequential approach is <b>independent of choice of prior and likelihood function and depends only on the assumption of i.i.d. data</b>.\n",
    "\n",
    "Let us say that our goal is to predict the outcome of the next trial based on the evidence that we have. This means we have to evaluate $p(x=1|D)$ which takes the form\n",
    "\n",
    "$$\\begin{align}\n",
    "p(x=1|D) = \\int_{0}^{1} p(x=1|\\mu)p(\\mu|D)d\\mu = \\int_{0}^{1} \\mu p(\\mu|D)d\\mu = E[\\mu|D]\n",
    "\\end{align}$$\n",
    "\n",
    "Using the mean of beta distribution, we get\n",
    "\n",
    "$$\\begin{align}\n",
    "p(x=1|D) = \\frac{m+a}{m+a+l+b}\n",
    "\\end{align}$$\n",
    "\n",
    "If we have infinitely large dataset (i.e. $m,l \\to \\infty$), above quantity reduces to the maximum likelihood estimate $a/(a+b)$. As the number of data point increases, the distribution becomes sharply peaked with variance decreasing. For $a,b \\to \\infty$, the variance $Var[\\mu] = \\frac{ab}{(a+b)^2(a+b+1)} \\to 0$. This is a general property of Bayesian learning. As we observe more and more data, the uncertaininty in posterior distribution will steadily decrease. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Multinomial Variables\n",
    "\n",
    "A <b>multinomila variable</b> can be in any of $K$ states instead of just $2$ (in case of a binary variable). For example, a multinomial variable having $K=5$ states can be represented as $x=(0,0,1,0,0)^T$ where it is in a state where $x_3=1$. This vector will satisfy $\\sum_{k=1}^K = 1$. If we denote the probability of $x_k=1$ by $\\mu_k$, the distribution of $x$ is given as:\n",
    "\n",
    "$$\\begin{align}\n",
    "p(x|\\mu) = \\prod_{k=1}^{K} \\mu_k^{x_k} \n",
    "\\end{align}$$\n",
    "\n",
    "where $\\mu = (\\mu_1, \\mu_2, ..., \\mu_k)^T$ and $\\mu_k \\geq 0$ and $\\sum_{k}\\mu_k = 1$. This distribution can be considered as the generalization of bernoulli distribution for more than $2$ outcomes. This distribution is normalized as (only one of the $x_k=1$ for all $x$):\n",
    "\n",
    "$$\\begin{align}\n",
    "\\sum_{x}p(x|\\mu) = \\sum_{x} \\bigg( \\prod_{k=1}^{K} \\mu_k^{x_k} \\bigg) = \\sum_{k=1}^{K} \\mu_k = 1\n",
    "\\end{align}$$\n",
    "\n",
    "Expected value of $x$ is given by\n",
    "\n",
    "$$\\begin{align}\n",
    "E[x|\\mu] = \\sum_{x}p(x|\\mu)x = \\sum_{x} \\bigg( \\prod_{k=1}^{K} \\mu_k^{x_k} \\bigg)x = (\\mu_1, \\mu_2, ..., \\mu_M)^T = \\mu\n",
    "\\end{align}$$\n",
    "\n",
    "Consider a dataset $D$ of $N$ independent observations $x_1, x_2, ..., x_N$. The corresponding likelihood function is given as:\n",
    "\n",
    "$$\\begin{align}\n",
    "p(D|\\mu) = \\prod_{n=1}^{N}\\prod_{k=1}^{K} \\mu_{k}^{x_{nk}}\n",
    "\\end{align}$$\n",
    "\n",
    "where $x_{nk}$ is the $k^{th}$ state of $n^{th}$ data point. The expression reduces further to\n",
    "\n",
    "$$\\begin{align}\n",
    "p(D|\\mu) = \\prod_{n=1}^{N}\\prod_{k=1}^{K} \\mu_{k}^{x_{nk}} = \\prod_{k=1}^{K} \\mu_{k}^{(\\sum_{n}x_{nk})} = \\prod_{k=1}^{K} \\mu_{k}^{m_k}\n",
    "\\end{align}$$\n",
    "\n",
    "where $m_k$ represents the number of observations for which $x_k=1$ and is\n",
    "\n",
    "$$\\begin{align}\n",
    "m_k = \\sum_{n}x_{nk}\n",
    "\\end{align}$$\n",
    "\n",
    "These are called the <b>sufficient statistics</b> for this distribution. In order to find the maximum likelihood estimate for $\\mu$, we need to maximize $\\ln p(D|\\mu)$ with respect to $\\mu_k$ with a constraint $\\sum_{k}\\mu_k = 1$. This can be achieved using a Lagrange multiplier $\\lambda$ and maximizing\n",
    "\n",
    "$$\\begin{align}\n",
    "\\sum_{k=1}^{K}m_k \\ln \\mu_{k} + \\lambda \\bigg( \\sum_{k}^{K}\\mu_k - 1\\bigg)\n",
    "\\end{align}$$\n",
    "\n",
    "Taking derivative with respect to $\\mu_{k}$ and equating it to $0$, we get\n",
    "\n",
    "$$\\begin{align}\n",
    "\\frac{m_k}{\\mu_{k}}  + \\lambda = 0 \\implies \\mu_{k} = \\frac{-m_k}{\\lambda}\n",
    "\\end{align}$$\n",
    "\n",
    "Substituting the value of $\\mu_k$ into the constraint $\\sum_{k}\\mu_k = 1$, we get\n",
    "\n",
    "$$\\begin{align}\n",
    "\\sum_{k}^{K}\\frac{-m_k}{\\lambda} = 1 \\implies \\frac{-1}{\\lambda}\\sum_{k}^{K}m_k = 1\n",
    "\\end{align}$$\n",
    "\n",
    "$$\\begin{align}\n",
    "\\implies \\frac{-1}{\\lambda}N = 1 \\implies \\lambda = -N\n",
    "\\end{align}$$\n",
    "\n",
    "Hence, we get\n",
    "\n",
    "$$\\begin{align}\n",
    "\\mu_{k}^{ML} = \\frac{m_k}{N}\n",
    "\\end{align}$$\n",
    "\n",
    "which is the fraction of the $N$ observations for which $x_k=1$. The <b>multinomial distribution</b> which is the distribution of the quantities $m_1,m_2, ..., m_K$ conditioned on the parameters $\\mu,N$ is given as:\n",
    "\n",
    "$$\\begin{align}\n",
    "Mult(m_1,m_2,...,m_K|\\mu,N) = {N \\choose m_1m_2...m_K} \\prod_{k=1}^{K} \\mu_k^{m_k}\n",
    "\\end{align}$$\n",
    "\n",
    "The <b>normalization coefficient</b> is number of ways of partitioning $N$ objects into $K$ groups of size $m_1,m_2,...,m_K$ and is given as\n",
    "\n",
    "$$\\begin{align}\n",
    "{N \\choose m_1m_2...m_K} = \\frac{N!}{m_1!m_2!...m_K!}\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.1 Dirichlet Distribution\n",
    "\n",
    "The <b>conjugate prior</b> for the multinomial distribution takes the form\n",
    "\n",
    "$$\\begin{align}\n",
    "p(\\mu|\\alpha) \\propto \\prod_{k=1}^{K} \\mu_{k}^{\\alpha_k - 1}\n",
    "\\end{align}$$\n",
    "\n",
    "where $0 \\leq \\mu_k \\leq 1$; $\\sum_{k} \\mu_k = 1$ and $\\alpha_1,\\alpha_2,...,\\alpha_k$ are the parameters of the distribution with $\\alpha = (\\alpha_1,\\alpha_2,...,\\alpha_k)^T$. This distribution is called as <b>Dirichlet distribution</b>. It is a mutivariate generalization of beta distribution and hence is also called as <b>multivariate beta distribution</b>. Due to the summation constraint, the ditribution over the space of the $\\{\\mu_k\\}$ is confined to a <b>simplex</b> of dimensionality $K-1$. For $K=3$, the illustration is shown in the following figure. For the case $K=3$, we have $\\{\\mu_1, \\mu_2,\\mu_3\\}$ with constraint $\\mu_k \\geq 0$ and $\\mu_1+\\mu_2+\\mu_3=1$. These constraints confine the values of $\\mu_k$ is the plane shown below.\n",
    "\n",
    "<img src=\"dirichlet.png\">\n",
    "\n",
    "Parameter $\\alpha$ governs the shape of the distribution inside the simplex. In particular the sum $\\alpha_0 = \\sum_{k}\\alpha_{k}$ controls the strength of the distribution (how peaked it is). If $\\alpha_k < 1$ for all $k$, we get <b>spikes at the corners of the simplex</b>. For values of $\\alpha_k > 1$, the distribution tends toward the centre of the simplex. As $\\alpha_0$ increases, the distribution becomes more tightly concentrated around the centre of the simplex. [Source: https://towardsdatascience.com/dirichlet-distribution-a82ab942a879]\n",
    "\n",
    "<img src=\"dirichlet_2.png\">\n",
    "\n",
    "The normalized form of the Dirichlet distribution is\n",
    "\n",
    "$$\\begin{align}\n",
    "Dir(\\mu|\\alpha) = \\frac{\\Gamma(\\alpha_0)}{\\Gamma(\\alpha_1)...\\Gamma(\\alpha_K)} \\prod_{k=1}^{K} \\mu_{k}^{\\alpha_k - 1}\n",
    "\\end{align}$$\n",
    "\n",
    "where $\\Gamma(x)$ is the gamma function and\n",
    "\n",
    "$$\\begin{align}\n",
    "\\alpha_{0} = \\sum_{k=1}^{K}\\alpha_{k}\n",
    "\\end{align}$$\n",
    "\n",
    "The <b>posterior distribution</b> for the parameter $\\{\\mu_k\\}$ takes the form\n",
    "\n",
    "$$\\begin{align}\n",
    "p(\\mu|D,\\alpha) \\propto p(D|\\mu)Dir(\\mu|\\alpha) \\propto \\prod_{k=1}^{K} \\mu_{k}^{\\alpha_k + m_k - 1}\n",
    "\\end{align}$$\n",
    "\n",
    "The normalization coefficient can be obtained by comparion and the final posterior distribution takes the form\n",
    "\n",
    "$$\\begin{align}\n",
    "p(\\mu|D,\\alpha) = Dir(\\mu|\\alpha + m) = \\frac{\\Gamma(\\alpha_0+N)}{\\Gamma(\\alpha_1+m_1)...\\Gamma(\\alpha_K+m_k)}\\prod_{k=1}^{K} \\mu_{k}^{\\alpha_k + m_k - 1}\n",
    "\\end{align}$$\n",
    "\n",
    "where $m=(m_1,m_2,...,m_K)^T$. The parameter $\\alpha_k$ can be interpreted as the effective number of observations of $x_k=1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 The Gaussian Distribution\n",
    "\n",
    "<b>Gaussian Distribution</b> is a widely used model for the distribution of continous variables. For a single variable $x$, the gaussian distribution is given as\n",
    "\n",
    "$$\\begin{align}\n",
    "N(x|\\mu,\\sigma^2) = \\frac{1}{(2\\pi\\sigma^2)^{1/2}} exp \\bigg[\\frac{-1}{2\\sigma^2} (x-\\mu)^2\\bigg]\n",
    "\\end{align}$$\n",
    "\n",
    "where $\\mu$ and $\\sigma^2$ are mean and variance respectively. For a $D$ dimensional vector $X$, the multivariate gaussian distribution takes the form\n",
    "\n",
    "$$\\begin{align}\n",
    "N(X|\\mu,\\Sigma) = \\frac{1}{(2\\pi)^{D/2}}\\frac{1}{|\\Sigma|^{1/2}} exp \\bigg[\\frac{-1}{2} (X-\\mu)^T\\Sigma^{-1}(X-\\mu)\\bigg]\n",
    "\\end{align}$$\n",
    "\n",
    "where $\\mu$ is a $D$ dimensional mean vector and $\\Sigma$ is a $D\\times D$ dimensional covariance matrix with $|\\Sigma|$ being its determinant.\n",
    "\n",
    "One of the major application of Normal/Gaussian distribution is in <b>Central Limit Theorem</b>. It states that <b>the distribution of sample means approximates a gaussian distribution as the sample size gets larger, regardless of the population's distribution</b>. \n",
    "\n",
    "Gaussian distribution is functionally dependent on $X$ is through the <b>quadratic form</b> which appears in the exponent shown below.\n",
    "\n",
    "$$\\begin{align}\n",
    "\\Delta^2 = (X-\\mu)^T\\Sigma^{-1}(X-\\mu)\n",
    "\\end{align}$$\n",
    "\n",
    "The quantity $\\Delta$ is called the <b>Mahalnobis distance</b> from $\\mu$ to $X$ which reduces to <b>Euclidean distance</b> when $\\Sigma$ is <b>identity matrix</b>. For the surface in $X$-space on which this quadratic form is constant, the gaussian distribution will be constant. Another thing to note is: $\\Sigma$ is a <b>real symmetric matrix</b>. Being a real symmetric matrix, its <b>eigenvalues will be real</b> and its <b>eigenvectors will form an orthonormal set</b>. The eigenvector equation can be given as:\n",
    "\n",
    "$$\\begin{align}\n",
    "\\Sigma u_i = \\lambda_i u_i\n",
    "\\end{align}$$\n",
    "\n",
    "where $u_i$ are eigenvectors with $\\lambda_i$ being the corresponding eigenvalues. The condition of orthonormality of eigenvectors give us\n",
    "\n",
    "$$\\begin{align}\n",
    "u_i^Tu_j = I_{ij}\n",
    "\\end{align}$$\n",
    "\n",
    "where $I_{ij} = 1$ $\\forall i=j$ and $0$ otherwise. The covariance matrix $\\Sigma$ can be represented in terms of eigenvalues and eigenvectors as\n",
    "\n",
    "$$\\begin{align}\n",
    "\\Sigma = \\sum_{i=1}^{D}\\lambda_iu_iu_i^T\n",
    "\\end{align}$$\n",
    "\n",
    "The inverse covaraince matrix $\\Sigma^{-1}$ can be expressed as\n",
    "\n",
    "$$\\begin{align}\n",
    "\\Sigma^{-1} = \\sum_{i=1}^{D}(\\lambda_iu_iu_i^T)^{-1} = \\sum_{i=1}^{D}\\frac{1}{\\lambda_i}(u_iu_i^T)^{-1} = \\sum_{i=1}^{D}\\frac{1}{\\lambda_i}u_iu_i^T\n",
    "\\end{align}$$\n",
    "\n",
    "Substituting it into the quadratic form, it reduces to\n",
    "\n",
    "$$\\begin{align}\n",
    "\\Delta^2 = (X-\\mu)^T \\bigg[ \\sum_{i=1}^{D} \\frac{1}{\\lambda_i}u_iu_i^T\\bigg](X-\\mu) = \\sum_{i=1}^{D} \\frac{y_i^2}{\\lambda_i}\n",
    "\\end{align}$$\n",
    "\n",
    "The derivation of above expression is shown below.\n",
    "\n",
    "<img src=\"quadratic_form.jpg\">\n",
    "\n",
    "$y_i$ can be interpreted as the original $x_i$ coordinate shifted by mean $\\mu_i$ and rotated to align the eigenvector $u_i$. In the new coordinate system, $Y = U(X-\\mu)$ where $U$ is the transpose of eigenvector matrix of the covariance matrix and is orthogonal. The representation of rotated gaussina distribution is shown below. If all the eigenvalues $\\lambda_i$ are positive, the surface of constant density is an <b>ellipsoid</b> with their centers at $\\mu$ and axes along $u_i$. For the Gaussian distribution to be well defined, all the eigenvalues $\\lambda_i$ shoulb be positive which makes the covariance matrix <b>positive definite</b>.\n",
    "\n",
    "<img src=\"rotated_gaussian.png\">\n",
    "\n",
    "Using the eigenvalue decomposition of covariance matrix, its determinant is given as\n",
    "\n",
    "$$\\begin{align}\n",
    "|\\Sigma|^{1/2} = \\prod_{j=1}^{D}(\\lambda_j)^{1/2}\n",
    "\\end{align}$$\n",
    "\n",
    "In the new coordinate system, eigenvectors being orthonormal, the Gaussian distribution takes the form of $D$ independent univariate gaussian distributions. \n",
    "\n",
    "$$\\begin{align}\n",
    "p(Y) = \\prod_{j=1}^{D}\\frac{1}{(2\\pi\\lambda_j)^{1/2}} exp\\bigg[\\frac{-y_j^2}{2\\lambda_j}\\bigg]\n",
    "\\end{align}$$\n",
    "\n",
    "Expectation of $X$ under the Gaussian distribution can be derived as\n",
    "\n",
    "$$\\begin{align}\n",
    "E[X] = \\frac{1}{(2\\pi)^{D/2}}\\frac{1}{|\\Sigma|^{1/2}} \\int exp \\bigg[\\frac{-1}{2} (X-\\mu)^T\\Sigma^{-1}(X-\\mu)\\bigg]XdX\n",
    "\\end{align}$$\n",
    "\n",
    "Using $Z=X-\\mu$, we get $dX=dZ$ and the expression reduces to\n",
    "\n",
    "$$\\begin{align}\n",
    "E[X] = \\frac{1}{(2\\pi)^{D/2}}\\frac{1}{|\\Sigma|^{1/2}} \\int exp \\bigg[\\frac{-1}{2} Z^T\\Sigma^{-1}Z\\bigg](Z+\\mu)dZ\n",
    "\\end{align}$$\n",
    "\n",
    "$$\\begin{align}\n",
    "= \\frac{1}{(2\\pi)^{D/2}}\\frac{1}{|\\Sigma|^{1/2}} \\bigg(\\int exp \\bigg[\\frac{-1}{2} Z^T\\Sigma^{-1}Z\\bigg]ZdZ + \\int exp \\bigg[\\frac{-1}{2} Z^T\\Sigma^{-1}Z\\bigg]\\mu dZ\\bigg) \n",
    "\\end{align}$$\n",
    "\n",
    "$$\\begin{align}\n",
    "= \\frac{1}{(2\\pi)^{D/2}}\\frac{1}{|\\Sigma|^{1/2}} \\bigg(0 + \\int exp \\bigg[\\frac{-1}{2} Z^T\\Sigma^{-1}Z\\bigg]\\mu dZ\\bigg) \n",
    "\\end{align}$$\n",
    "\n",
    "$$\\begin{align}\n",
    "= \\mu\\frac{1}{(2\\pi)^{D/2}}\\frac{1}{|\\Sigma|^{1/2}} \\int exp \\bigg[\\frac{-1}{2} Z^T\\Sigma^{-1}Z\\bigg] dZ = \\mu\n",
    "\\end{align}$$\n",
    "\n",
    "The <b>second order moment</b> of a univariate Gaussian is given as $E[x^2]$. For a multivariate Gaussian, this is defined as $E[XX^T]$ and is computed as\n",
    "\n",
    "$$\\begin{align}\n",
    "E[XX^T] = \\frac{1}{(2\\pi)^{D/2}}\\frac{1}{|\\Sigma|^{1/2}} \\int exp \\bigg[\\frac{-1}{2} (X-\\mu)^T\\Sigma^{-1}(X-\\mu)\\bigg]XX^TdX\n",
    "\\end{align}$$\n",
    "\n",
    "$$\\begin{align}\n",
    "= \\frac{1}{(2\\pi)^{D/2}}\\frac{1}{|\\Sigma|^{1/2}} \\int exp \\bigg[\\frac{-1}{2} Z^T\\Sigma^{-1}Z\\bigg](Z+\\mu)(Z+\\mu)^TdZ\n",
    "\\end{align}$$\n",
    "\n",
    "$$\\begin{align}\n",
    "= \\frac{1}{(2\\pi)^{D/2}}\\frac{1}{|\\Sigma|^{1/2}} \\int exp \\bigg[\\frac{-1}{2} Z^T\\Sigma^{-1}Z\\bigg](ZZ^T + \\mu Z^T + Z\\mu^T + \\mu\\mu^T)dZ\n",
    "\\end{align}$$\n",
    "\n",
    "The terms with $\\mu Z^T,Z\\mu^T$ will vanish as the expression inside the integral is an odd function. $\\mu\\mu^T$ being constant, the integral for the expression having it will be $\\mu\\mu^T$ (as the Gaussian distribution is normalized). The integral of the term with $ZZ^T$ will come out to be $\\Sigma$. Hence,\n",
    "\n",
    "$$\\begin{align}\n",
    "E[XX^T] = \\mu\\mu^T + \\Sigma\n",
    "\\end{align}$$\n",
    "\n",
    "This gives us the covariance as\n",
    "\n",
    "$$\\begin{align}\n",
    "Cov[X] = \\Sigma\n",
    "\\end{align}$$\n",
    "\n",
    "Total number of independent parameters in a multivariate Gaussian will depend on the dimension $D$ of the dataset. We will have a total of $D$ independet parameters in mean $\\mu$. For the covariance matrix $\\Sigma$, we will have a total of ${D \\choose 2} = \\frac{D(D+1)}{2}$ independent parameters. Hence, in the problem of density estimation, we have to come with an estimation of a total of $\\frac{D(D+3)}{2}$ parameters. Using restrictive form of covariance matrix reduces the number of independent parameters drastically. For a diagonal covariance matrix, we will have a total of $2D$ parameters to estimate with the contours of constant density as axis-aligned ellipsoids. If we further restrict the covariance matrix to be proportional to identity matrix, we will have a total of $D+1$ independent parameters for estimation and we will get the spherical surface of constant density. The shape of desities for these $3$ cases is shown in the figure below. Restricting the covariance matrix makes the calculation simplified but but it limits the number of degree of freedom in the distribution and hence limits its ability to capture interesting correlation in the dataset.\n",
    "\n",
    "<img src=\"gaussian_contours.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.1 Conditional Gaussian Distribution\n",
    "\n",
    "If two sets of variables are jointly Gaussian, then the conditional distribution of one set conditioned on the othre is Gaussian. The marginal distribution of either set is also Gaussian. Let $X$ is a $D$-dimensional vector with Gaussian distribution $N(X|\\mu,\\Sigma)$. $X$ is partitioned into two disjoint subsets $X_a,X_b$. Without loss of generality we can assume thet $X_a$ forms the first $M$ components of $X$ and $X_b$ the remaining $D-M$, such that\n",
    "\n",
    "$$\\begin{align}\n",
    "X = \\begin{pmatrix}\n",
    "X_a\\\\\n",
    "X_b\n",
    "\\end{pmatrix};\\mu = \\begin{pmatrix}\n",
    "\\mu_a\\\\\n",
    "\\mu_b\n",
    "\\end{pmatrix};\\Sigma = \\begin{pmatrix}\n",
    "\\Sigma_{aa} & \\Sigma_{ab}\\\\\n",
    "\\Sigma_{ba} & \\Sigma_{bb}\n",
    "\\end{pmatrix}\n",
    "\\end{align}$$\n",
    "\n",
    "As $\\Sigma$ is symmetric, i.e. $\\Sigma^T = \\Sigma$. This implies that $\\Sigma_{aa} = \\Sigma_{bb}$ and $\\Sigma_{ba} = \\Sigma_{ab}^T$. It is eaasier to work with the inverse of the covarince matrix, which is called as <b>precision matrix</b> $\\Lambda = \\Sigma^{-1}$. The partitioned form of precision matrix is given as\n",
    "\n",
    "$$\\begin{align}\n",
    "\\Lambda = \\begin{pmatrix}\n",
    "\\Lambda_{aa} & \\Lambda_{ab}\\\\\n",
    "\\Lambda_{ba} & \\Lambda_{bb}\n",
    "\\end{pmatrix}\n",
    "\\end{align}$$\n",
    "\n",
    "where $\\Lambda_{aa},\\Lambda_{bb}$ are symmetric and $\\Lambda_{ba} = \\Lambda_{ab}^T$. As we know that the Gaussian ditribution is of the quadratic form with respect to the input $X$, it will be sufficient to show that the quadratic form of joint Gaussian when partitioned into $X_a,X_b$ and conditioned on $X_b$ ($X_a$ is variable with fixed $X_b$) takes the quadratic form for $X_a$. \n",
    "\n",
    "$$\\begin{align}\n",
    "\\Delta^2 = -\\frac{1}{2}(X-\\mu)^T\\Sigma^{-1}(X-\\mu)\n",
    "\\end{align}$$\n",
    "\n",
    "$$\\begin{align}\n",
    "= -\\frac{1}{2}\\begin{pmatrix}\n",
    "X_a-\\mu_a\\\\\n",
    "X_b-\\mu_b\n",
    "\\end{pmatrix}^T\\begin{pmatrix}\n",
    "\\Lambda_{aa} & \\Lambda_{ab}\\\\\n",
    "\\Lambda_{ba} & \\Lambda_{bb}\n",
    "\\end{pmatrix}\\begin{pmatrix}\n",
    "X_a-\\mu_a\\\\\n",
    "X_b-\\mu_b\n",
    "\\end{pmatrix}\n",
    "\\end{align}$$\n",
    "\n",
    "$$\\begin{align}\n",
    "= -\\frac{1}{2}(X_a-\\mu_a)^T\\Lambda_{aa}(X_a-\\mu_a) -\\frac{1}{2}(X_a-\\mu_a)^T\\Lambda_{ab}(X_b-\\mu_b)\n",
    "\\end{align}$$\n",
    "$$\\begin{align}\n",
    "-\\frac{1}{2}(X_b-\\mu_b)^T\\Lambda_{ba}(X_a-\\mu_a) -\\frac{1}{2}(X_b-\\mu_b)^T\\Lambda_{bb}(X_b-\\mu_b)\n",
    "\\end{align}$$\n",
    "\n",
    "The above expresssion as a function of $X_a$ takes a quadratic form and hence the corresponding conditional distribution $p(X_a|X_b)$ is Gaussian.\n",
    "\n",
    "One way to find the mean $\\mu$ and the covariance/precision matirx $\\Sigma$ is to compute the coefficients in the quadratic form of Gaussian. The quadractic form of Gaussian can be further decomposed as\n",
    "\n",
    "$$\\begin{align}\n",
    "\\Delta^2 = -\\frac{1}{2}(X-\\mu)^T\\Sigma^{-1}(X-\\mu) = -\\frac{1}{2}X^T\\Sigma^{-1}X + X^T\\Sigma^{-1}\\mu + const\n",
    "\\end{align}$$\n",
    "\n",
    "Hence, from the above expressed quadratic form, we can compute $\\Sigma^{-1}$ by computing the coefficint of second order term of $X$ and using the linear term of $X$, we can get $\\Sigma^{-1}\\mu$ from which we can obtain mean $\\mu$.\n",
    "\n",
    "Considering the expanded quadratic form of conditional distribution given above, the second order term of $X_a$ is $-\\frac{1}{2}X_a^T\\Lambda_{aa}X_a$. From this we can conclude that the covariance matrix for conditional distribution is given as\n",
    "\n",
    "$$\\begin{align}\n",
    "\\Sigma_{a|b} = \\Lambda_{aa}^{-1}\n",
    "\\end{align}$$\n",
    "\n",
    "Linear term in $X_a$ is\n",
    "\n",
    "$$\\begin{align}\n",
    "X_a^T[\\Lambda_{aa}\\mu_a - \\Lambda_{ab}(X_b - \\mu_b)]\n",
    "\\end{align}$$\n",
    "\n",
    "The coefficient of $X_a$ must equal $\\Sigma_{a|b}^{-1}\\mu_{a|b}$ and hence\n",
    "\n",
    "$$\\begin{align}\n",
    "\\Sigma_{a|b}^{-1}\\mu_{a|b} = \\Lambda_{aa}\\mu_a - \\Lambda_{ab}(X_b - \\mu_b)\n",
    "\\end{align}$$\n",
    "\n",
    "$$\\begin{align}\n",
    "\\mu_{a|b} = \\Sigma_{a|b}\\bigg(\\Lambda_{aa}\\mu_a - \\Lambda_{ab}(X_b - \\mu_b)\\bigg)\n",
    "\\end{align}$$\n",
    "\n",
    "$$\\begin{align}\n",
    "= \\Lambda_{aa}^{-1}\\bigg(\\Lambda_{aa}\\mu_a - \\Lambda_{ab}(X_b - \\mu_b)\\bigg) = \\mu_a - \\Lambda_{aa}^{-1}\\Lambda_{ab}(X_b - \\mu_b) \n",
    "\\end{align}$$\n",
    "\n",
    "Using matrix algebra, we can express the mean and covariance of conditional distribution in termes of partitioned mean and covariance matrix of joint distribution as\n",
    "\n",
    "$$\\begin{align}\n",
    "\\mu_{a|b} = \\mu_{a} + \\Sigma_{ab}\\Sigma_{bb}^{-1}(X_b - \\mu_b)\n",
    "\\end{align}$$\n",
    "\n",
    "$$\\begin{align}\n",
    "\\Sigma_{a|b} = \\Sigma_{aa} - \\Sigma_{ab}\\Sigma_{bb}^{-1}\\Sigma_{ba}\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.2 Marginal Gaussian Distribution\n",
    "\n",
    "The <b>marginal distribution</b> of a joint Gaussian, given as\n",
    "\n",
    "$$\\begin{align}\n",
    "p(X_a) = \\int p(X_a,X_b)dX_b\n",
    "\\end{align}$$\n",
    "\n",
    "is also Gaussian. It can be shown using the similar approach which is used for condition distribution above. The mean and covariance of marginal distribution is given as:\n",
    "\n",
    "$$\\begin{align}\n",
    "E[X_a] = \\mu_a\n",
    "\\end{align}$$\n",
    "\n",
    "$$\\begin{align}\n",
    "Cov[X_a] = \\Sigma_{aa}\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.3 Bayes' Theorem for Gaussian Variables\n",
    "\n",
    "The applicatio of Bayes' theorem in the Gaussian setting is when we have Gaussian marginal distribution $p(X)$ and a Gaussian conditional distribution $p(Y|X)$ and we wish to find the marginal and consitional distribution $p(Y)$ and $p(X|Y)$. It should be noted that the mean and covariace of the marginal distribution $p(X)$ are constant with respect to $X$. The covarince of the conditional distribution $p(Y|X)$ is constant with respect to $X$ but its mean is a <b>linear function</b> of $X$. Hence, the marginal can consitional distribution can be represented as\n",
    "\n",
    "$$\\begin{align}\n",
    "p(X) = N(X|\\mu,\\Lambda^{-1})\n",
    "\\end{align}$$\n",
    "\n",
    "$$\\begin{align}\n",
    "p(Y|X) = N(Y|AX+b,L^{-1})\n",
    "\\end{align}$$\n",
    "\n",
    "where $\\mu,A,b$ are parameters governing the mean and $\\Lambda,L$ are the precision matrices.\n",
    "\n",
    "Let the joint distribution over $X,Y$, defined as $Z$ such that\n",
    "\n",
    "$$\\begin{align}\n",
    "Z = \\begin{pmatrix}\n",
    "X\\\\\n",
    "Y\n",
    "\\end{pmatrix}\n",
    "\\end{align}$$\n",
    "\n",
    "From Bayes' theorem, $p(Z) = p(X,Y) = p(X)p(Y|X)$. This means that\n",
    "\n",
    "$$\\begin{align}\n",
    "\\ln p(Z) = \\ln p(X) + \\ln p(Y|X)\n",
    "\\end{align}$$\n",
    "\n",
    "$$\\begin{align}\n",
    "= -\\frac{1}{2}(X-\\mu)^T\\Lambda(X-\\mu) -\\frac{1}{2}(Y-AX-b)^TL(Y-AX-b) + const\n",
    "\\end{align}$$\n",
    "\n",
    "The above expression is a quadratic form of the componnets of $Z$ and hence the joint distribution is Gaussian. Its mean and precison matrix ($R$) can be computed using the coefficients of the quadratic form of $Z$ (i.e. $\\begin{pmatrix}\n",
    "X\\\\\n",
    "Y\n",
    "\\end{pmatrix}$)in a similar way and are given as\n",
    "\n",
    "$$\\begin{align}\n",
    "R = \\begin{pmatrix}\n",
    "\\Lambda + A^TLA & -A^TL\\\\\n",
    "-LA & L\n",
    "\\end{pmatrix}\n",
    "\\end{align}$$\n",
    "\n",
    "$$\\begin{align}\n",
    "Cov(Z) =R^{-1} = \\begin{pmatrix}\n",
    "\\Lambda^{-1} & \\Lambda^{-1}A^T\\\\\n",
    "A\\Lambda^{-1} & L^{-1}+A\\Lambda^{-1}A^T\n",
    "\\end{pmatrix}\n",
    "\\end{align}$$\n",
    "\n",
    "$$\\begin{align}\n",
    "E[Z] = R^{-1}\\begin{pmatrix}\n",
    "A\\mu-A^TLb\\\\\n",
    "Lb\n",
    "\\end{pmatrix} = \\begin{pmatrix}\n",
    "\\mu\\\\\n",
    "A\\mu+b\n",
    "\\end{pmatrix}\n",
    "\\end{align}$$\n",
    "\n",
    "This joint distribution can be used to find the parameters for the marginal and conditional distribution $p(Y)$ and p(X|Y). For the marginal distribution $p(Y)$, the mean and covariance are given as\n",
    "\n",
    "$$\\begin{align}\n",
    "E[Y] = A\\mu+b\n",
    "\\end{align}$$\n",
    "\n",
    "$$\\begin{align}\n",
    "Cov[Y] = L^{-1}+A\\Lambda^{-1}A^T\n",
    "\\end{align}$$\n",
    "\n",
    "For the conditional distribution $p(X|Y)$, the mean and covariance are given as\n",
    "\n",
    "$$\\begin{align}\n",
    "Cov[X|Y] = (\\Lambda + A^TLA)^{-1}\n",
    "\\end{align}$$\n",
    "\n",
    "$$\\begin{align}\n",
    "E[Y] = (\\Lambda + A^TLA)^{-1} \\bigg(A^TL(Y-b) + \\Lambda\\mu \\bigg)\n",
    "\\end{align}$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.4 Maximum Likelihood for the Gaussian\n",
    "\n",
    "For a dataset $X = (X_1, X_2,...,X_N)^T$ where the individual observations $X_n$ are assumed to be drawn independetly from a mutivariate Gaussian, the log likelihood function is given as\n",
    "\n",
    "$$\\begin{align}\n",
    "\\ln p(X|\\mu,\\Sigma) = -\\frac{ND}{2}\\ln(2\\pi) -\\frac{N}{2}\\ln(|\\Sigma|) -\\frac{1}{2}\\sum_{n=1}^{N}(X_n-\\mu)^T\\Sigma^{-1}(X_n-\\mu)\n",
    "\\end{align}$$\n",
    "\n",
    "Taking the derivative of the log likelihood function with respect to $\\mu$, we get\n",
    "\n",
    "$$\\begin{align}\n",
    "\\frac{\\delta}{\\delta\\mu}\\ln p(X|\\mu,\\Sigma) = \\sum_{n=1}^{N}\\Sigma^{-1}(X_n-\\mu)\n",
    "\\end{align}$$\n",
    "\n",
    "To compute the derivative, this explanation is used [Source: https://math.stackexchange.com/questions/312077/differentiate-fx-xtax]:\n",
    "\n",
    "Calculate the differential of the function $f: \\Bbb R^n \\to \\Bbb R$ given by $$f(x) = x^T A x$$ with $A$ symmetric. Also, differentiate this function with respect to $x^T$.\n",
    "\n",
    "As a start, things work \"as usual\": You calculate the difference between $f(x+h)$ and $f(x)$ and check how it depends on $h$, looking for a dominant linear part as $h\\to 0$.\n",
    "Here, $f(x+h)=(x+h)^TA(x+h)=x^TAx+ h^TAx+x^TAh+h^TAh=f(x)+2x^TAh+h^TAh$, so $f(x+h)-f(x)=2x^TA\\cdot h + h^TAh$. The first summand is linear in $h$ with a factor $2x^TA$, the second summand is quadratic in $h$, i.e. goes to $0$ faster than the first / is negligible against the first for small $h$. So the row vector $2x^TA$ is our derivative (or transposed: $2Ax$ is the derivative with respect to $x^T$).\n",
    "\n",
    "Computing the derivative to $0$, we get the maximum likelihood estimator of mean as\n",
    "\n",
    "$$\\begin{align}\n",
    "\\mu_{ML} = \\frac{1}{N}\\sum_{n=1}^{N}X_n\n",
    "\\end{align}$$\n",
    "\n",
    "Similarly, the maximum likelihood estimator of covariance can be computed as\n",
    "\n",
    "$$\\begin{align}\n",
    "\\Sigma_{ML} = \\frac{1}{N}\\sum_{n=1}^{N}(X_n-\\mu_{ML})(X_n-\\mu_{ML})^T\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.5 Sequential Estimation\n",
    "\n",
    "In a <b>sequential estimation</b> process, data points are processed one at a time and then discareded. This process is important for on-line applications and for large data sets. For maximum likelihood estimator for the mean $\\mu_{ML}$, the updated mean after dissecting out contribution for the final data point $x_N$ is\n",
    "\n",
    "$$\\begin{align}\n",
    "\\mu_{ML}^{(N)} = \\mu_{ML}^{(N-1)} + \\frac{1}{N}(x_N - \\mu_{ML}^{(N-1)})\n",
    "\\end{align}$$\n",
    "\n",
    "The new mean can be interpreted as moving the old mean in the direction of the <b>error signal</b> $(x_N - \\mu_{ML}^{(N-1)})$ by a small amount $1/N$. As $N$ inreases, the contribution from new data point gets smaller. There are more genric way to come up with a sequential learning algorithm when they can't be derived via this route."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.6 Bayesian Inference for the Gaussian\n",
    "\n",
    "We can use Bayesian treatment to derive the point estimates for the mean and variance of the Gaussian by introducing prior distributions over these parameters. For a single ranodm variable, let us suppose that the variance $\\sigma^2$ is known and we have to determine the mean $\\mu$ given $N$ data points $X=\\{x_1,x_2,...,x_N\\}$. Under the assumption of independence, the likelihood function is give as\n",
    "\n",
    "$$\\begin{align}\n",
    "p(X|\\mu) = \\prod_{n=1}^{N} p(x_n|\\mu) = \\frac{1}{(2\\pi\\sigma^2)^{N/2}} exp \\bigg(-\\frac{1}{2\\sigma^2}\\sum_{n=1}^{N}(x_n - \\mu)^2\\bigg)\n",
    "\\end{align}$$\n",
    "\n",
    "As the likelihood function takes the form of the exponential of a <b>quadratic form</b> in $\\mu$, we can choose a <b>conjugate prior</b> $p(\\mu)$ given by the Gaussian as\n",
    "\n",
    "$$\\begin{align}\n",
    "p(\\mu) = N(\\mu|\\mu_0, \\sigma_0^2)\n",
    "\\end{align}$$\n",
    "\n",
    "Posterior distribution is given as\n",
    "\n",
    "$$\\begin{align}\n",
    "p(\\mu|X) \\propto p(X|\\mu)p(\\mu)\n",
    "\\end{align}$$\n",
    "\n",
    "$$\\begin{align}\n",
    "p(\\mu|X) \\propto exp \\bigg(-\\frac{1}{2\\sigma^2}\\sum_{n=1}^{N}(x_n - \\mu)^2\\bigg)exp \\bigg(-\\frac{1}{2\\sigma_0^2}(\\mu - \\mu_0)^2\\bigg)\n",
    "\\end{align}$$\n",
    "\n",
    "$$\\begin{align}\n",
    "p(\\mu|X) \\propto exp \\bigg(\\sum_{n=1}^{N}\\bigg[-\\frac{1}{2\\sigma^2}(x_n - \\mu)^2 -\\frac{1}{2N\\sigma_0^2}(\\mu - \\mu_0)^2\\bigg]\\bigg)\n",
    "\\end{align}$$\n",
    "\n",
    "The quadratic form can be completed as\n",
    "\n",
    "$$\\begin{align}\n",
    "\\sum_{n=1}^{N}\\bigg[-\\frac{1}{2\\sigma^2}(x_n - \\mu)^2 -\\frac{1}{2N\\sigma_0^2}(\\mu - \\mu_0)^2\\bigg]\n",
    "\\end{align}$$\n",
    "\n",
    "$$\\begin{align}\n",
    "=\\sum_{n=1}^{N}\\bigg[-\\frac{1}{2\\sigma^2}(x_n^2 + \\mu^2 - 2x_n\\mu) -\\frac{1}{2N\\sigma_0^2}(\\mu^2 + \\mu_0^2 - 2\\mu\\mu_0)\\bigg]\n",
    "\\end{align}$$\n",
    "\n",
    "$$\\begin{align}\n",
    "=\\sum_{n=1}^{N}-\\frac{1}{2}\\bigg[\\bigg(\\frac{1}{\\sigma^2} + \\frac{1}{N\\sigma_0^2}\\bigg)\\mu^2   - 2\\bigg(\\frac{x_n}{\\sigma^2} + \\frac{\\mu_0}{N\\sigma_0^2}\\bigg)\\mu+ const\\bigg]\n",
    "\\end{align}$$\n",
    "\n",
    "$$\\begin{align}\n",
    "=-\\frac{1}{2}\\bigg[\\bigg(\\frac{N}{\\sigma^2} + \\frac{1}{\\sigma_0^2}\\bigg)\\mu^2   - 2\\bigg(\\sum_{n=1}^{N}\\frac{x_n}{\\sigma^2} + \\frac{\\mu_0}{\\sigma_0^2}\\bigg)\\mu+ const\\bigg]\n",
    "\\end{align}$$\n",
    "\n",
    "$$\\begin{align}\n",
    "=-\\frac{1}{2}\\bigg[\\bigg(\\frac{N}{\\sigma^2} + \\frac{1}{\\sigma_0^2}\\bigg)\\mu^2   - 2\\bigg(\\frac{N\\mu_{ML}}{\\sigma^2} + \\frac{\\mu_0}{\\sigma_0^2}\\bigg)\\mu + const\\bigg]\n",
    "\\end{align}$$\n",
    "\n",
    "Comparing the above expression with the generic quadratic form of Gaussian shown below, we can compute the mean and variance.\n",
    "\n",
    "$$\\begin{align}\n",
    "\\Delta^2 = -\\frac{1}{2}(X-\\mu)^T\\Sigma^{-1}(X-\\mu) = -\\frac{1}{2}X^T\\Sigma^{-1}X + X^T\\Sigma^{-1}\\mu + const\n",
    "\\end{align}$$\n",
    "\n",
    "Coefficient of second order term gives us the inverse of variance, i.e.\n",
    "\n",
    "$$\\begin{align}\n",
    "\\frac{1}{\\sigma_N^2} = \\frac{1}{\\sigma_0^2} + \\frac{N}{\\sigma^2} \n",
    "\\end{align}$$\n",
    "\n",
    "The coefficient of first order term gives us the product of mean and inverse of varince, i.e.\n",
    "\n",
    "$$\\begin{align}\n",
    "\\frac{\\mu_N}{\\sigma_N^2} = \\frac{N\\mu_{ML}}{\\sigma^2} + \\frac{\\mu_0}{\\sigma_0^2}\n",
    "\\end{align}$$\n",
    "\n",
    "$$\\begin{align}\n",
    "\\mu_N = \\sigma_N^2 \\bigg(\\frac{N\\mu_{ML}}{\\sigma^2} + \\frac{\\mu_0}{\\sigma_0^2}\\bigg) = \\frac{\\sigma_0^2\\sigma^2}{\\sigma^2+N\\sigma_0^2} \\bigg(\\frac{N\\mu_{ML}}{\\sigma^2} + \\frac{\\mu_0}{\\sigma_0^2}\\bigg)\n",
    "\\end{align}$$\n",
    "\n",
    "$$\\begin{align}\n",
    "\\mu_N = \\frac{\\sigma^2}{\\sigma^2+N\\sigma_0^2}\\mu_0 + \\frac{N\\sigma_0^2}{\\sigma^2+N\\sigma_0^2}\\mu_{ML}\n",
    "\\end{align}$$\n",
    "\n",
    "where \n",
    "\n",
    "$$\\begin{align}\n",
    "\\mu_{ML} = \\frac{1}{N}\\sum_{n=1}^{N}x_n\n",
    "\\end{align}$$\n",
    "\n",
    "The mean $\\mu_N$ of the postrior distribution is a compromise between <b>prior mean</b> and <b>MLE mean</b>. If $N = 0$, it reduces to prior mean and as $N \\to \\infty$, the posterior mean is given by the maximum likelihood mean $\\mu_{ML}$. If we look at the expression for inverse of the varince, i.e. <b>precision</b>, precison of the posterior is given by the precision of the prior plus one contribution of the data precision from each of the observed data points. Precision steadily increases which means that the variance steadily decreases as we see more and more data points. As $N \\to \\infty$, $\\sigma_N^2 \\to 0$ and hence the posterior distribution infinitely peaks around the maximum likelihood solution. Bayesian inference of mean is illustrated in the following figure.\n",
    "\n",
    "<img src=\"bayesian_gaussian.png\">\n",
    "\n",
    "Bayesian inference leads natrually to a sequential estimation process as the estimate of the mean can be expressed as follown. \n",
    "\n",
    "$$\\begin{align}\n",
    "p(\\mu|D) \\propto \\bigg[p(\\mu)\\prod_{n=1}^{N-1}p(x_n|\\mu)\\bigg]p(x_N|\\mu)\n",
    "\\end{align}$$\n",
    "\n",
    "The term in square bracket is the posterior distribution upto observing $N-1$ data points. This can be viewed as a prior distribution for $N^{th}$ data point which when combined with the likelihood function of $N^{th}$ data point (given as $p(x_N|\\mu)$) gives us the posterior distribution after observing $N$ data points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.7 Student's t-distribution\n",
    "\n",
    "<b>Student's t-distribution</b> is defined as\n",
    "\n",
    "$$\\begin{align}\n",
    "St(x|\\mu,\\lambda,v) = \\frac{\\Gamma(v/2+1/2)}{\\Gamma(v/2)} \\bigg(\\frac{\\lambda}{\\pi v}\\bigg)^{1/2} \\bigg[ 1 + \\frac{\\lambda(x-\\mu)^2}{v}\\bigg]^{-v/2-1/2}\n",
    "\\end{align}$$\n",
    "\n",
    "The parameter $v$ is called as the <b>degree of freedom</b> and its effect is illustrated in following figure. As $v\\to\\infty$, the t-distribution reduces to a Gaussian $N(x|\\mu,\\lambda^{-1})$ with mean $\\mu$ and precision $\\lambda$.\n",
    "\n",
    "<img src=\"t_distribution.png\">\n",
    "\n",
    "Student’s t-distribution is obtained by adding up an infinite number of Gaussian distributions having the same mean but different precisions. This can be interpreted as an infinite mixture of Gaussians.The result is a distribution that in general has longer ‘tails’ than a Gaussian, as seen in the above figure. This gives the tdistribution an important property called robustness, which means that it is much less sensitive than the Gaussian to the presence of a few data points which are <b>outliers</b>. Robustness to outliers withe example is shown in the following figure.\n",
    "\n",
    "<img src=\"t_distribution_outliers.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.8 Periodic Variables\n",
    "\n",
    "<b>Periodic variables</b> usually repeats its behaviour after a set amount of time. Let us assume a periodic variable represented as an angle from the x-axis by $D=\\{\\theta_1, \\theta_2, ..., \\theta_N\\}$, where $\\theta$ is measured in radians. The <b>mean and variance of these data points will depend on the choice of origin and the axis</b>. To find the invariant measure of the mean, we denote these observations as the points on the unit circle and can be described as a two-dimensional unit vectors $X_1,X_2,...,X_N$ where $||X_n|| = 1$ for $n=1,2,...,N$ as shown in the following figure.\n",
    "\n",
    "<img src=\"periodic_variable.png\">\n",
    "\n",
    "Mean in the new coordinate system is given as\n",
    "\n",
    "$$\\begin{align}\n",
    "\\bar{X} = \\frac{1}{N}\\sum_{n=1}^{N}X_n\n",
    "\\end{align}$$\n",
    "\n",
    "which can be used to find the mean $\\bar{\\theta}$. $\\bar{x}$ will typically lie inside the unit circle. The coordinates of the observation are given as $X_n = (\\cos \\theta_n, \\sin \\theta_n)$ and let the coordinates of the sample mean $\\bar{x}$ are $X_n = (\\bar{r}\\cos \\bar{\\theta}, \\bar{r}\\sin \\bar{\\theta})$. Then\n",
    "\n",
    "$$\\begin{align}\n",
    "\\bar{r}\\cos \\bar{\\theta} = \\frac{1}{N}\\sum_{n=1}^{N}\\cos\\theta_n\n",
    "\\end{align}$$\n",
    "\n",
    "$$\\begin{align}\n",
    "\\bar{r}\\sin \\bar{\\theta} = \\frac{1}{N}\\sum_{n=1}^{N}\\sin\\theta_n\n",
    "\\end{align}$$\n",
    "\n",
    "Using these equations, we get\n",
    "\n",
    "$$\\begin{align}\n",
    "\\bar{\\theta} = \\tan^{-1} \\bigg(\\frac{\\sum_{n}\\sin\\theta_n}{\\sum_{n}\\cos\\theta_n}\\bigg)\n",
    "\\end{align}$$\n",
    "\n",
    "Periodic generaization of Gaussian is called as <b>von Mises distribution</b>. Let us consider a distribution $p(\\theta)$ which has a period $2\\pi$. $p(\\theta)$ must satisfy these three conditions\n",
    "\n",
    "$$\\begin{align}\n",
    "p(\\theta) \\geq 0\n",
    "\\end{align}$$\n",
    "\n",
    "$$\\begin{align}\n",
    "\\int_{0}^{2\\pi} p(\\theta)d\\theta = 1\n",
    "\\end{align}$$\n",
    "\n",
    "$$\\begin{align}\n",
    "p(\\theta + M2\\pi) = p(\\theta)\n",
    "\\end{align}$$\n",
    "\n",
    "Now consider a Gaussian distribution over two variables $x=(x_1,x_2)$ having mean $\\mu=(\\mu_1,\\mu_2)$ and a covarince matrix $\\Sigma = \\sigma^2I$, then\n",
    "\n",
    "$$\\begin{align}\n",
    "p(x_1,x_2) = \\frac{1}{2\\pi\\sigma^2}exp\\bigg[ -\\frac{(x_1-\\mu_1)^2+(x_2-\\mu_2)^2}{2\\sigma^2}\\bigg]\n",
    "\\end{align}$$\n",
    "\n",
    "The contours of constant $p(x)$ will be circle as shown by blue concentric circles in the following figure. If we consider the value of this distribution along a circle of fixed radius, it will be periodic but not normalized.\n",
    "\n",
    "<img src=\"von_mises.png\">\n",
    "\n",
    "The distribution can be transformed to polar coordiates as $x_1 = r\\cos\\theta$, $x_2 = r\\sin\\theta$ and $\\mu_1 = r_0\\cos\\theta_0$,$\\mu_2 = r_0\\sin\\theta_0$. The exponent in the Gaussia then reduces to\n",
    "\n",
    "$$\\begin{align}\n",
    "-\\frac{(r\\cos\\theta-r_0\\cos\\theta_0)^2+(r\\sin\\theta-r_0\\sin\\theta_0)^2}{2\\sigma^2} = \\frac{r_0}{\\sigma^2}\\cos(\\theta-\\theta_0) + const\n",
    "\\end{align}$$\n",
    "\n",
    "Taking $m=r_0/\\sigma^2$, the distribution of $p(\\theta)$ along the unit circel $r=1$ is given as\n",
    "\n",
    "$$\\begin{align}\n",
    "p(\\theta|\\theta_0,m) = \\frac{1}{2\\pi I_0(m)} exp [m\\cos(\\theta - \\theta_0)]\n",
    "\\end{align}$$\n",
    "\n",
    "and is called as <b>von Mises distribution or circular normal distribution</b>. $\\theta_0$ correspond to the mean of the distribution and $m$ is called as the <b>concentartion parameter</b> and is analogous to variance. The <b>normalization coefficient</b> is given as\n",
    "\n",
    "$$\\begin{align}\n",
    "I_0(m) = \\frac{1}{2\\pi} \\int_{0}^{2\\pi} exp [m\\cos\\theta] d\\theta\n",
    "\\end{align}$$\n",
    "\n",
    "The maximum likelihood estimator for the parameter $\\theta_0$ and $m$ can be derived the same way. Conside the log likelihood function\n",
    "\n",
    "$$\\begin{align}\n",
    "\\ln p(D|\\theta_0,m) = - N\\ln(2\\pi) - N\\ln I_0(m) + m\\sum_{n=1}^{N}\\cos(\\theta_n - \\theta_0)\n",
    "\\end{align}$$\n",
    "\n",
    "Setting the derivative with respect to $\\theta_0$ equal to $0$, we get\n",
    "\n",
    "$$\\begin{align}\n",
    "\\sum_{n=1}^{N}\\sin(\\theta_n - \\theta_0) = 0\n",
    "\\end{align}$$\n",
    "\n",
    "Solving this equation, we get\n",
    "\n",
    "$$\\begin{align}\n",
    "\\theta_0^{ML} = \\tan^{-1}\\bigg[\\frac{\\sum_n\\sin\\theta_n}{\\sum_n\\cos\\theta_n} \\bigg]\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.9 Mixtures of Gaussians\n",
    "\n",
    "Linear combination of Gaussians can give rise to very complex densities. By using a sufficient number of Gaussians, and by adjusting their means and covariances as well as the coefficients in the linear combination, almost any continuous density can be approximated to arbitrary accuracy. Superposition of $K$ Gaussians of the form\n",
    "\n",
    "$$\\begin{align}\n",
    "p(X) = \\sum_{k=1}^{K} \\pi_{k} N(X|\\mu_k, \\Sigma_k)\n",
    "\\end{align}$$\n",
    "\n",
    "is called as <b>mixture of Gaussians</b>. Each Gaussian $N(X|\\mu_k, \\Sigma_k)$ is called as the <b>component</b> of the mixture and has its own mean and variance. The parameters $\\pi_k$ are called <b>mixing coefficicnts</b> such that $0 \\leq \\pi_k \\leq 1$ and should satisfy\n",
    "\n",
    "$$\\begin{align}\n",
    "\\sum_{k=1}^{K}\\pi_k = 1\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 The Exponential Family\n",
    "\n",
    "<b>Exponential family</b> of distributions over $X$ with parameters $\\eta$ is defined as\n",
    "\n",
    "$$\\begin{align}\n",
    "p(X|\\eta) = h(X)g(\\eta)exp[\\eta^Tu(X)]\n",
    "\\end{align}$$\n",
    "\n",
    "Here $\\eta$ is the <b>natural parameter</b> of the distribution and $u(X)$ is some function of $X$. $g(\\eta)$ ensures that the distribution is normalized and satisfies\n",
    "\n",
    "$$\\begin{align}\n",
    "g(\\eta) \\int h(X)exp[\\eta^Tu(X)] dX = 1\n",
    "\\end{align}$$\n",
    "\n",
    "<b>Bernoulli distribution</b> is a common exponential distribution. It is given as\n",
    "\n",
    "$$\\begin{align}\n",
    "p(x|\\mu) = Bern(x|\\mu) = \\mu^x(1-\\mu)^{1-x} = exp[x\\ln \\mu + (1-x)\\ln(1-\\mu)]\n",
    "\\end{align}$$\n",
    "\n",
    "$$\\begin{align}\n",
    "= exp\\bigg[x\\ln \\bigg(\\frac{\\mu}{1-\\mu}\\bigg) + \\ln(1-\\mu)\\bigg] = (1-\\mu) exp\\bigg[x\\ln \\bigg(\\frac{\\mu}{1-\\mu}\\bigg)\\bigg]\n",
    "\\end{align}$$\n",
    "\n",
    "Comparing this with the expression for the exponetial family of distribution, we get\n",
    "\n",
    "$$\\begin{align}\n",
    "\\eta = \\ln \\bigg(\\frac{\\mu}{1-\\mu}\\bigg)\n",
    "\\end{align}$$\n",
    "\n",
    "Or, we can say that\n",
    "\n",
    "$$\\begin{align}\n",
    "\\mu = \\sigma(\\eta) = \\frac{1}{1+exp(-\\eta)}\n",
    "\\end{align}$$\n",
    "\n",
    "which is called as the <b>logistic sigmoid function</b>. Hence,\n",
    "\n",
    "$$\\begin{align}\n",
    "1-\\mu = 1-\\sigma(\\eta) = 1-\\frac{1}{1+exp(-\\eta)} = \\frac{exp(-\\eta)}{1+exp(-\\eta)} = \\frac{1}{1+exp(\\eta)} = \\sigma(-\\eta)\n",
    "\\end{align}$$\n",
    "\n",
    "Finally, the Bernoulli distribution can be represented as\n",
    "\n",
    "$$\\begin{align}\n",
    "p(x|\\eta) = \\sigma(-\\eta)exp(\\eta x)\n",
    "\\end{align}$$\n",
    "\n",
    "Comparing it with the general expression on the exponential distribution, we get $u(x) = x$, $h(x) = 1$ and $g(\\eta) = \\sigma(-\\eta)$.\n",
    "\n",
    "Another example of exponential distribution is <b>multinomial distribution</b>. For a single observation $X$, it takes the form\n",
    "\n",
    "$$\\begin{align}\n",
    "p(X|\\mu) = \\prod_{k=1}^{M}\\mu_k^{x_k} = exp\\bigg[ \\sum_{k=1}^{M} x_k \\ln \\mu_k\\bigg] = exp(\\eta^TX)\n",
    "\\end{align}$$\n",
    "\n",
    "where $\\eta = (\\eta_1, \\eta_2, ..., \\eta_M)^T$ and $\\eta_k = \\ln\\mu_k$. Comparing with the general equation of exponential family, we get $u(x) = X$, $h(x) = 1$ and $g(\\eta) = 1$. It should be noted that the parameters $\\eta_k$ are not indepndet as $\\sum_k \\mu_k = 1$. Hence, $\\mu_M$ can be derived from rest of the values as:\n",
    "\n",
    "$$\\begin{align}\n",
    "\\mu_M = 1 - \\sum_{k=1}^{M-1}\\mu_k\n",
    "\\end{align}$$\n",
    "\n",
    "Using this, the multinomial distribution can be rewritten as\n",
    "\n",
    "$$\\begin{align}\n",
    "p(X|\\mu) = exp\\bigg[ \\sum_{k=1}^{M} x_k \\ln \\mu_k\\bigg] = exp\\bigg[ \\sum_{k=1}^{M-1} x_k \\ln \\mu_k + x_M \\ln \\mu_M\\bigg]\n",
    "\\end{align}$$\n",
    "\n",
    "$$\\begin{align}\n",
    "= exp\\bigg[ \\sum_{k=1}^{M-1} x_k \\ln \\mu_k + \\bigg(1 - \\sum_{k=1}^{M-1}x_k\\bigg) \\ln\\bigg(1 - \\sum_{k=1}^{M-1}\\mu_k\\bigg)\\bigg]\n",
    "\\end{align}$$\n",
    "\n",
    "$$\\begin{align}\n",
    "= exp\\bigg[ \\sum_{k=1}^{M-1} x_k \\ln\\bigg(\\frac{\\mu_k}{1 - \\sum_{j=1}^{M-1}\\mu_j}\\bigg) + \\ln\\bigg(1 - \\sum_{k=1}^{M-1}\\mu_k\\bigg)\\bigg]\n",
    "\\end{align}$$\n",
    "\n",
    "$$\\begin{align}\n",
    "= \\bigg(1 - \\sum_{k=1}^{M-1}\\mu_k\\bigg)exp\\bigg[ \\sum_{k=1}^{M-1} x_k \\ln\\bigg(\\frac{\\mu_k}{1 - \\sum_{j=1}^{M-1}\\mu_j}\\bigg)\\bigg]\n",
    "\\end{align}$$\n",
    "\n",
    "Comparing this with the general expression for exponential distribution, we get \n",
    "\n",
    "$$\\begin{align}\n",
    "\\eta_k = \\ln\\bigg(\\frac{\\mu_k}{1 - \\sum_{j=1}^{M-1}\\mu_j}\\bigg)\n",
    "\\end{align}$$\n",
    "\n",
    "Solving for $\\mu_k$, we get\n",
    "\n",
    "$$\\begin{align}\n",
    "\\mu_k = \\frac{exp(\\eta_k)}{1+\\sum_j exp(\\eta_j)}\n",
    "\\end{align}$$\n",
    "\n",
    "which is called as <b>softmax function or normalized exponential</b>. In this form, the multinomial distribution can be represented as\n",
    "\n",
    "$$\\begin{align}\n",
    "p(X|\\eta) = \\bigg(1 + \\sum_{k=1}^{M-1}exp(\\eta_k)\\bigg)^{-1}exp(\\eta^TX)\n",
    "\\end{align}$$\n",
    "\n",
    "<b>Gaussian</b> can also be reduced in the form of exponetial distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4.1 Maximum Likelihood and Sufficient Statistics\n",
    "\n",
    "<b>Exponential family</b> of distributions over $X$ with parameters $\\eta$ satisfies\n",
    "\n",
    "$$\\begin{align}\n",
    "g(\\eta) \\int h(X)exp[\\eta^Tu(X)] dX = 1\n",
    "\\end{align}$$\n",
    "\n",
    "Taking derivative with respect to $\\eta$, we have\n",
    "\n",
    "$$\\begin{align}\n",
    "\\nabla g(\\eta) \\int h(X)exp[\\eta^Tu(X)] dX + g(\\eta) \\int h(X)exp[\\eta^Tu(X)]u(X) dX= 0\n",
    "\\end{align}$$\n",
    "\n",
    "$$\\begin{align}\n",
    "-\\frac{1}{g(\\eta)}\\nabla g(\\eta) = \\frac{\\int h(X)exp[\\eta^Tu(X)]u(X) dX}{\\int h(X)exp[\\eta^Tu(X)]dX}\n",
    "\\end{align}$$\n",
    "\n",
    "$$\\begin{align}\n",
    "-\\frac{1}{g(\\eta)}\\nabla g(\\eta) = g(\\eta) \\int h(X)exp[\\eta^Tu(X)]u(X) dX = E[u(X)]\n",
    "\\end{align}$$\n",
    "\n",
    "Hence,\n",
    "\n",
    "$$\\begin{align}\n",
    "E[u(X)] = -\\nabla \\ln g(\\eta)\n",
    "\\end{align}$$\n",
    "\n",
    "The maximum likelihood estimator of $\\eta$, denoted as $\\eta_{ML}$ will satisfy\n",
    "\n",
    "$$\\begin{align}\n",
    "-\\nabla \\ln g(\\eta_{ML}) = \\frac{1}{N}\\sum_{n=1}^{N}u(X_n)\n",
    "\\end{align}$$\n",
    "\n",
    "The solution of the maximum likelihood estimator depends on the data through $\\sum_{n=1}u(X_n)$ only, which is therefore called as <b>sufficient statistic</b> of the distribution. It should be noted that <b>we don't need to store the entire dataset itself but just the value of sufficient statistc</b>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4.2 Noninformative Priors\n",
    "\n",
    "In many cases, we will have little or no idea how the prior distribution should look like. We may then seek a form of prior distribution, called a <b>noninformative prior</b>, which is intended to have as little influence on the posterior distribution as possible. \n",
    "\n",
    "For a distribution $p(x|\\lambda)$, we might be tempted to propose a prior distribution $p(\\lambda) = const$. For a discrete $\\lambda$ with $K$ states, we can set the prior probability of each state as $1/K$. For continuous $\\lambda$, if the domain of $\\lambda$ is <b>unbounded</b>, the prior distribution cannot be correctly normalized as the integral over $\\lambda$ diverges. Such priors are called <b>improper</b>. Another prolem is with the non-linear transformation of the variable. Let's say we want change the variable $\\lambda$ as $\\lambda = \\eta^2$. The new density is then give as\n",
    "\n",
    "$$\\begin{align}\n",
    "p_{\\eta}(\\eta) = p_{\\lambda}(\\lambda)\\bigg|\\frac{d\\lambda}{d\\eta}\\bigg| = p_{\\lambda}(\\eta^2)2\\eta\n",
    "\\end{align}$$\n",
    "\n",
    "And hence the new density will not be constant.\n",
    "\n",
    "Let us say that a density with the parameter $\\mu$, called as the <b>location parameter</b>, takes a form\n",
    "\n",
    "$$\\begin{align}\n",
    "p(x|\\mu) = f(x-\\mu)\n",
    "\\end{align}$$\n",
    "\n",
    "This family of density shows <b>translational invariance</b> as for a shifted $x$, $\\hat{x} = x+c$, the new density is\n",
    "\n",
    "$$\\begin{align}\n",
    "p(\\hat{x}|\\hat{\\mu}) = f(\\hat{x}-\\hat{\\mu})\n",
    "\\end{align}$$\n",
    "\n",
    "where $\\hat{\\mu} = \\mu+c$. This means that the density is independent of choice of origin. The density which satisfies the translational invariance property assign equal probability mass to the interval $A \\leq \\mu \\leq B$ and the shifted interval $A-c \\leq \\mu \\leq B-c$, i.e.\n",
    "\n",
    "$$\\begin{align}\n",
    "\\int_{A}^{B}p(\\mu)d\\mu = \\int_{A-c}^{B-c}p(\\mu)d\\mu = \\int_{A}^{B}p(\\mu-c)d\\mu\n",
    "\\end{align}$$\n",
    "\n",
    "This should hold for all choices of $A,B$ and hence $p(\\mu-c) = p(\\mu)$ which implies that $p(\\mu)$ is constant.\n",
    "\n",
    "Let us consider another form of density which is defined as\n",
    "\n",
    "$$\\begin{align}\n",
    "p(x|\\sigma) = \\frac{1}{\\sigma}f\\bigg(\\frac{x}{\\sigma}\\bigg)\n",
    "\\end{align}$$\n",
    "\n",
    "where $\\sigma > 0$. The parameter $\\sigma$ is known as <b>scale parameter</b> and the density exhibits <b>scale invariance</b> as when $x$ is scaled by a factor such that $\\hat{x} = cx$, then\n",
    "\n",
    "$$\\begin{align}\n",
    "p(\\hat{x}|\\hat{\\sigma}) = \\frac{1}{\\hat{\\sigma}}f\\bigg(\\frac{\\hat{x}}{\\hat{\\sigma}}\\bigg)\n",
    "\\end{align}$$\n",
    "\n",
    "where $\\hat{\\sigma} = c\\sigma$. For scale invariance, the probability for the inervals $A \\leq \\sigma \\leq B$ and $A/c \\leq \\sigma \\leq B/c$ should be same, i.e.\n",
    "\n",
    "$$\\begin{align}\n",
    "\\int_{A}^{B}p(\\sigma)d\\sigma = \\int_{A/c}^{B/c}p(\\sigma)d\\sigma = \\int_{A}^{B}p\\bigg( \\frac{\\sigma}{c}\\bigg)\\frac{1}{c}d\\sigma\n",
    "\\end{align}$$\n",
    "\n",
    "For this to hold\n",
    "\n",
    "$$\\begin{align}\n",
    "p(\\sigma) = p\\bigg( \\frac{\\sigma}{c}\\bigg)\\frac{1}{c}\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 Nonparametric Methods\n",
    "\n",
    "Till now to model the data, we have choosen distributions which are governed by some fixed small number of parameters. This is called as <b>parametric approach</b> to density modeling. The chosen density might be a poor model of the distribution that generates the data, which can result in poor predictive performance. In <b>nonparametric approach</b>, we make fewer assumptions about the form of distribution.\n",
    "\n",
    "One common nonparametric approach is <b>histogram density models</b>. In this approach, we partition $x$ into bins of width $\\Delta_i$ and then count the number of observations falling in the bins (denoted as $n_i$). To have the normalized probability distribution, we can calculate the probability for eac bin as\n",
    "\n",
    "$$\\begin{align}\n",
    "p_i = \\frac{n_i}{N\\Delta_i}\n",
    "\\end{align}$$\n",
    "\n",
    "where $N$ is the total number of observations and $\\int p(x)dx = 1$. An illustration of histogram density model is shown below.\n",
    "\n",
    "<img src=\"histogram_density_model.png\">\n",
    "\n",
    "For smaller value of $\\Delta$, the resulting density model is spiky with a lot of structure that is not present in the underlying distribution. For larger $\\Delta$, the resultant model is too smooth and misses the bimodal property of underlying distribution. The best result is obtained by some intermediate value of $\\Delta$. One of the advantage of histogram density models is that once the histogram has been computed, we can discard the dataset. One of the obvious disadvantage of this model is that the estimated densities have discontinuties due to the bin edges. Another major limitation of the histogram approach is its scaling with dimensionality. If we divide each variable in a $D$-dimensional space into $M$ bins, then the total number of bins will be $M^D$. This exponential scaling with $D$ is an example of the curse of dimensionality. \n",
    "\n",
    "Histogram density estimation approach teaches us following important technique though:\n",
    "\n",
    "* First, to estimate the probability density at a particular location, we should consider the data points that lie within some local neighbourhood of that point. Note that the concept of locality requires that we assume some form of distance measure, and here we have been assuming <b>Euclidean distance</b>.\n",
    "\n",
    "* Second, the neighbourhood property was defined by the bins, and there is a natural ‘smoothing’ parameter describing the spatial extent of the local region, in this case the bin\n",
    "width. The value of the smoothing parameter should be neither too large nor too small in order to obtain good results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5.1 Kernel Density Estimators\n",
    "\n",
    "Let us suppose that observations are being drawn from some unknown probability density $p(x)$ in some $D$-dimensional space. From our earlier discussion of locality, let us consider some small region $R$ containing $x$. The probability mass associated with this region is given by\n",
    "\n",
    "$$\\begin{align}\n",
    "P = \\int_{R} p(x)dx\n",
    "\\end{align}$$\n",
    "\n",
    "Now suppose that we have collected a data set comprising $N$ observations drawn from $p(x)$. Because each data point has a probability $P$ of falling within $R$, the total number $K$ of points that lie inside $R$ will be distributed according to the binomial distribution\n",
    "\n",
    "$$\\begin{align}\n",
    "Bin(K|N,P) = \\frac{N!}{K!(N-K)!}P^K(1-P)^{1-K}\n",
    "\\end{align}$$\n",
    "\n",
    "The mean fraction of points falling inside this region is $E[K/N]=P$ with a variance of $Var[K/N]=P(1-P)/N$. As $N \\to \\infty$, the distributio will be sharply peaked around the mean and hence $K \\simeq NP$. For a sufficiently small region $R$ with volume $V$, the probability density $p(x)$ is roughly constant over the region and then, we have\n",
    "\n",
    "$$\\begin{align}\n",
    "P \\simeq p(x)V\n",
    "\\end{align}$$\n",
    "\n",
    "From the above two equations, we get the density estimate as\n",
    "\n",
    "$$\\begin{align}\n",
    "p(x) = \\frac{K}{NV}\n",
    "\\end{align}$$\n",
    "\n",
    "We can exploit this equation in two ways:\n",
    " * We can fix $K$ and determine the value of $V$ from the data. This is called $K-nearest-neighbour$ technique.\n",
    "\n",
    " * We can fix $V$ and determine $K$ from the data. This is called as <b>kernal approach</b>.\n",
    "\n",
    "For the <b>kernal approach</b>, let $R$ be the small hypercube centred at $x$ at which we have to determine the density. A <b>unit cube</b> centred around the origin can be given as\n",
    "\n",
    "$$\\begin{align}\n",
    "    k(u)= \n",
    "\\begin{cases}\n",
    "    1, & |u_i| \\leq 1/2;i=1,2,...,D\\\\\n",
    "    0, & \\text{otherwise}\n",
    "\\end{cases}\n",
    "\\end{align}$$\n",
    "\n",
    "$k(u)$ is an example of <b>kernel function</b>. For a datapoint $x_n$ the quantity $k((x-x_n)/h)$ will be $1$ if it lies inside the cube of side $h$ centred on $x$ and $0$ otherwise. The total number of datapoints lying inside this cube will be\n",
    "\n",
    "$$\\begin{align}\n",
    "K = \\sum_{n=1}^{N} k\\bigg( \\frac{x-x_n}{h} \\bigg)\n",
    "\\end{align}$$\n",
    "\n",
    "The estimated density is given as\n",
    "\n",
    "$$\\begin{align}\n",
    "p(x) = \\frac{1}{N}\\sum_{n=1}^{N} \\frac{1}{h^D} k\\bigg( \\frac{x-x_n}{h} \\bigg)\n",
    "\\end{align}$$\n",
    "\n",
    "as the volume $V=h^D$. Kernel density estimator suffers from the same problem of discontinuties at the boundries. A smoother density model is obtained when a smoother kernel function is used. <b>Gaussian Kernel</b> is one of the choices, which gives rise to the follwing density model\n",
    "\n",
    "$$\\begin{align}\n",
    "p(x) = \\frac{1}{N}\\sum_{n=1}^{N}\\frac{1}{(2\\pi h^2)^{1/2}} exp\\bigg[ - \\frac{||x-x_n||^2}{2h^2}\\bigg]\n",
    "\\end{align}$$\n",
    "\n",
    "where $h$ represents the standard deviation of the Gaussian components. Thus our density model is obtained by placing a Gaussian over each data point and then adding up the contributions over the whole data set, and then dividing by $N$ so that the density is correctly normalized. Tha result of Gaussian kernel density estimation is shown below. It can be seen that the optimization of $h$ determines how fit the density model is to the dataset.\n",
    "\n",
    "<img src=\"gaussian_density_model.png\">\n",
    "\n",
    "It should be noted that any kernel function $k(u)$ can be chosen for the density estimation task subject to the conditions\n",
    "\n",
    "$$\\begin{align}\n",
    "k(u) \\geq 0\n",
    "\\end{align}$$\n",
    "\n",
    "$$\\begin{align}\n",
    "\\int k(u) du = 1\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5.2 Nearest-neighbours Method\n",
    "\n",
    "One of the drawbacks of kernel approach is that $h$ is fixed irrespective of data density in the regions. In regions of high data density, a large value of $h$ may lead to over-smoothing and a washing out of structure that might otherwise be extracted from the data. However, reducing $h$ may lead to noisy estimates elsewhere in data space where the density is smaller. In <b>Nearest-neighbours Method</b>, we fix the $K$ and determine the value of $V$ from the data. To do this, we consider a small sphere centred at $x$ at which we have to estimate the density $p(x)$ and we allow the sphere to grow until it contains precisely $K$ points. The density is then given by the following equation with $V$ set as the volume of the sphere.\n",
    "\n",
    "$$\\begin{align}\n",
    "p(x) = \\frac{K}{NV}\n",
    "\\end{align}$$\n",
    "\n",
    "The value of $K$ now governs the degree of smoothing and that again there is an optimum choice for\n",
    "$K$ that is neither too large nor too small. Note that the model produced by $K$ nearest neighbours is not a true density model because the integral over all space diverges.\n",
    "\n",
    "The $K$-nearest-neighbour technique for density estimation can be extended to the problem of classification. Let us suppose that we have a data set comprising $N_k$ points in class $C_k$ with $N$ points in total, so that $\\sum_{k}N_k = N$. If we wish to classify a new point $x$, we draw a sphere centred on $x$ containing precisely $K$ points irrespective of their class. Suppose this sphere has volume $V$ and contains $K_k$ points of class $C_k$. The estimate of density associated with each class is given as\n",
    "\n",
    "$$\\begin{align}\n",
    "p(x|C_k) = \\frac{K_k}{N_kV}\n",
    "\\end{align}$$\n",
    "\n",
    "The unconditional density is given as\n",
    "\n",
    "$$\\begin{align}\n",
    "p(x) = \\frac{K}{NV}\n",
    "\\end{align}$$\n",
    "\n",
    "and the class priors as\n",
    "\n",
    "$$\\begin{align}\n",
    "p(C_k) = \\frac{N_k}{N}\n",
    "\\end{align}$$\n",
    "\n",
    "From Bayes' theorem, we get\n",
    "\n",
    "$$\\begin{align}\n",
    "p(C_k|x) = \\frac{p(X|C_k)p(C_k)}{p(x)} = \\frac{K_k}{K}\n",
    "\\end{align}$$\n",
    "\n",
    "The probability of misclassification can be minimized by assigning the point to the class with highest posterior probability (i.e. the one which has the largest value of $K_k/K$). Here $K$ controls the degree of smoothness. It should be noted that these nonparametric methods are still severely limited."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
